<!DOCTYPE html>
<html lang="en">
<head>
  <!-- GA4 -->
  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-GVRZX76T54"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-GVRZX76T54');
</script>
  <!---->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Crop recommendation based on the soil and weather properties</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      line-height: 1.6;
      background-color: #f4f4f4;
    }
    header {
      background: #4CAF50;
      color: white;
      padding: 10px 0;
      text-align: center;
    }
    nav {
      background: #333;
      overflow: hidden;
    }
    nav a {
      float: left;
      display: block;
      color: white;
      text-align: center;
      padding: 14px 20px;
      text-decoration: none;
      cursor: pointer;
    }
    nav a:hover {
      background: #ddd;
      color: black;
    }
    .tabcontent {
      display: none;
      padding: 20px;
      margin: 20px;
      background: white;
      border-radius: 5px;
      box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
    }
    .image-container {
      text-align: center;
      margin: 20px 0;
    }
    img {
    max-width: 500px; /* Set maximum width to 256px */
    max-height: auto; /* Set maximum height to 256px */
    width: auto; /* Maintain aspect ratio */
    height: auto; /* Maintain aspect ratio */
    border: 2px solid #ddd; /* Optional: Add a border */
    border-radius: 5px; /* Optional: Add rounded corners */
    box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2); /* Optional: Add shadow */
  }
  </style>
</head>
<body>
  <header>
    <h1>Crop recommendation based on the soil and weather properties</h1>
  </header>
  
  <nav>
    <a onclick="showTab('introduction')">Introduction</a>
    <a onclick="showTab('data-prep')">Data Prep/EDA</a>
    <a onclick="showTab('pca')">PCA</a>
    <a onclick="showTab('clustering')">Clustering</a>
    <a onclick="showTab('arm')">ARM</a>
    <a onclick="showTab('naive-bayes')">Naive Bayes</a>
    <a onclick="showTab('decision-tree')">Decision Trees</a>
    <a onclick="showTab('regression')">Regression</a>
  </nav>
  
  <!-- Introduction Tab -->
  <div id="introduction" class="tabcontent">
    <h3>Introduction</h3>

     <h3>Author: Sujith Battu</h3>
    <h4>Github code: <a href="https://github.com/larry6683/ML"></a>https://github.com/larry6683/ML</h4>
    <p>
      Agriculture is among the cornerstones of human civilization, supplying food, raw materials, and income for billions of human beings worldwide. Nonetheless, with growing demands brought by climate change, soil degradation, as well as a globally surging population, agricultural producers are met with new challenges in maximizing crop yields. In solving the problems, data-driven solutions have come as realistic steps in supporting agriculture's decision-making. Taking into consideration the soil's condition, climate, as well as other environmental factors, machine-learning algorithms can forecast the most suitable crop for a given area, thus allowing sustainable agricultural production as well as maximum production level.
      This study explores the development of a smart crop recommendation system based on advanced machine learning techniques to enable data-driven decision-making in agriculture. By integrating multiple datasets of soil characteristics, weather patterns, and historical crop yields, the system can identify and analyze complex patterns in the agricultural environment. 
      The approach employs clustering and predictive modeling techniques to assess site-specific conditions accurately, leading to tailored crop recommendations that optimize both yield potential and resource use. 
      By transcending traditional heuristic methods, the study demonstrates how a modern, systematic analysis of agronomic data can yield accurate and adaptable recommendations that are transferable to local conditions.
<br><br>
      Experimental verifications indicate that the machine learning–based method is more precise and consistent than conventional crop selection methods and hence of tremendous practical utility to farmers and agricultural stakeholders.
      The model's capacity to process multivariate inputs also provides valuable insights into the impact of individual factors—such as soil pH, nutrient content, and climatic variability—on crop performance. Such insights not only render the model more interpretable but also enable sustainable land management and optimal production strategies. 
      Overall, this research suggests the revolutionary potential of machine learning in precision agriculture, with implications for technology-facilitated strategies for maximizing sustainable, resilient, and economically viable crop production.


    </p>
    <a href="https://data.mendeley.com/datasets/8v757rr4st/1">Dataset source: https://data.mendeley.com/datasets/8v757rr4st/1</a></br>
- I downloaded the dataset from the Elsevier's Mendeley data repository through an unofficial API that is available to fetch data related to crops.
      <h3>Modelling</h3>
    <p>
      This project leverages three unsupervised data mining techniques—clustering, association rule mining (ARM), and principal component analysis (PCA)—to uncover hidden patterns in an agricultural dataset and enhance crop recommendations based on soil and weather properties. 
      It begins by grouping data points using clustering methods such as K-means, hierarchical clustering, and DBSCAN while carefully addressing data preparation challenges like missing values and normalization, and by evaluating distance metrics (e.g., Euclidean and cosine similarity) to determine the best way to capture similarities among samples. 
      The study then employs ARM through the Apriori algorithm to extract frequent itemsets and association rules, assessing these rules with measures like support, confidence, and lift to reveal which features tend to occur together. Finally, PCA is applied to reduce the dimensionality of the data while retaining the majority of its inherent variance, thereby simplifying the dataset for further analysis. 
      Grounded in an extensive literature review on key factors such as soil texture, pH levels, and weather attributes, the work also examines the raw state of the data and its subsequent transformation, acknowledging limitations due to measurement noise and biases. Overall, the integrated approach generates complementary insights that not only enrich the technical understanding of agronomic variables but also have practical implications for improved crop management and decision-making in agriculture.
      </p>
    <p>
      
The project is centered on the use of supervised methods such as Logistic Regression and Multinomial Naive Bayes in coming up with a crop recommendation system. The models are trained on a dataset of soil and weather characteristics to estimate the most appropriate crops for a particular set of factors. Logistic Regression, being interpretable as well as simple, is well suited for binary classification problems, whereas Multinomial Naive Bayes is well suited to classify categorical variables and hence perfectly suited for agricultural data.
Through a performance comparison of these two models, the project will identify the benefits and constraints of each for agricultural data analysis. Performance will be compared based on the use of confusion matrices as well as accuracy scores and other performance measures. Through this, we aim to contribute significantly to precision agriculture practice by helping growers improve productivity as well as sustainability through informed decision-making based on available data.
    </p>
<p>
    <div class="image-container">
      <img src="https://storage.googleapis.com/kaggle-datasets-images/1046158/1760012/9c08835b1e7e1084728b5202089fb65b/dataset-cover.jpg?t=2020-12-19-05-11-01" alt="">
    </div>

</p>
    <p>
        <b>10 Research Gaps/ Questions</b>
        <ol>
           <li>The following are ten questions that this data can answer:
            <li>How do variations in soil nutrients (N, P, K) influence the appropriateness of specific crops?
              <li>What are the most frequently recommended crops in high rather than low rainfall?
                <li>How is ambient temperature related to crop choice in different regions?
                  <li>How do soil pH levels correlate with the recommended crops?
                    <li>How does relative humidity influence the effectiveness of certain levels of soil nutrients on crop appropriateness?
            <li>Are there some combinations of climatic and soil conditions that always favor some crops?
              <li>Can crop recommendations be deduced from seasonal trends in environmental conditions?
                <li>How do recorded environmental readings affect the expected crop selection, and what might that say about regional agriculture?
            <li>Which crops appear to be most resilient across a broad spectrum of conditions in the data set? Why does a small variation in any one factor (e.g., a small increase in temperature) lead to a different crop recommendation?    


    </p>
</div>

<!-- Data Preparation/EDA -->
  <div id="data-prep" class="tabcontent">
    <h3>Data Overview</h3>
    <p>
      1. Data Collection<br>
The dataset was uploaded as a CSV file (cleaned_crop_data.csv). It contains the following features:<br>
- Soil properties: Soilcolor, Ph, K, P, N, Zn, S.<br>
- Environmental factors: QV2M-W, QV2M-Sp, QV2M-Su, QV2M-Au, T2M_MAX-W, T2M_MAX-Sp, T2M_MAX-Su, T2M_MAX-Au, T2M_MIN-W, T2M_MIN-Sp, T2M_MIN-Su, T2M_MIN-Au, PRECTOTCORR-W, PRECTOTCORR-Sp, PRECTOTCORR-Su, PRECTOTCORR-Au, WD10M, GWETTOP, CLOUD_AMT, WS2M_RANGE, PS.<br>
- Target variable: label (crop type).<br>
- Derived features: Ph_binned, N_P_interaction.<br>
<br>
2. Raw Data Overview<br>
2.1. Displaying Raw Data<br>
Below is a small portion of the raw data:<br>
brown	5.4	231	4.0	0.1333	1.0	16.0	8.50	25.34	Barley
    </p><br>
    <h3>2.2. Data Preparation/EDA</h3>
    <p>
      Exploratory Data Analysis (EDA) Description for the Crop recommendation dataset is a critical step in understanding the structure, patterns, and relationships within the dataset.
    </p>
    <h3>Visualizations</h3>

    <!-- Visualization 1 -->
    <div class="image-container">
      <p><strong>Figure 1:</strong> Original Dataframe</p>
      <img src="images\1.png" alt="Original dataframe">

    </div>

    <!-- Visualization 2 -->
    <div class="image-container">
      <p><strong>Figure 2:</strong> Dataset after cleaning process</p>
      <img src="images\2.png" alt="Cleaned Dataset">

    </div>

    <!-- Visualization 3 -->
    <div class="image-container">
      <img src="images\3.png" alt="Missing Values Heatmap">
      <p><strong>Figure 3:</strong> Few Images from the dataset.</p>
    </div>

    <!-- Visualization 4 -->
    <div class="image-container">
      <img src="images\4.png" alt="Boxplot for Outliers">
      <p><strong>Figure 4:</strong> Class Distribution of Land Cover Images Dataset.</p>
    </div>

    <!-- Visualization 5 -->
    <div class="image-container">
      <img src="images\5.png" alt="Pairplot of Numerical Features">
      <p><strong>Figure 5:</strong> Classes in the Dataset.</p>
    </div>

    <!-- Visualization 6 -->
    <div class="image-container">
      <img src="images\6.png" alt="Countplot of Categorical Features">
      <p><strong>Figure 6:</strong> Aspect Ration Distribution of Images.</p>
    </div>

    <!-- Visualization 7 -->
    <div class="image-container">
      <img src="images\7.png" alt="Pie Chart of Class Proportions">
      <p><strong>Figure 7:</strong> Scatter plot of Image size distribution among the Dataset.</p>
    </div>

    <!-- Visualization 8 -->
    <div class="image-container">
      <img src="images\8.png" alt="KDE Plot of Numerical Features">
      <p><strong>Figure 8:</strong> Average Image size per class.</p>
    </div>

    <!-- Visualization 9 -->
    <div class="image-container">
      <img src="images\9.png" alt="Violin Plot of Class-Wise Distribution">
      <p><strong>Figure 9:</strong>  Brightness distriubtion of IMages in Dataset.</p>
    </div>

    <!-- Visualization 10 -->
    <div class="image-container">
      <img src="images\10.png" alt="Scatter Plot of Relationships">
      <p><strong>Figure 10:</strong>Images from each class of the dataset.</p>
    </div>
    <!-- Data used for Modelling -->
<div class="image-container">
  <img src="images/36.jpeg" alt="Scatter Plot of Relationships">
  <p><strong></strong>Cleaned dataframe used to perform supervised learning models.</p>
</div>

<p><b>
According to the analysis or EDA I observed that the dataset is now clean and all the 7530 images in the dataset are perfectly processed.
</b></p>

  </div>
</div>

<!-- PCA-->
<div id="pca" class="tabcontent">
  <h3>Principle Component Analysis (PCA)</h3>
  <p>
    <b>Overview:</b><br>
    Principal Components
    First Principal Component (PC1): Captures the maximum variance in the data 
    . For example, in a dataset with correlated features (e.g., height and weight), PC1 would align with the direction where data points spread out the most.
    Second Principal Component (PC2): Explains the next highest variance uncorrelated with PC1 and orthogonal to it. Subsequent components follow this pattern but account for progressively less variance.
    Key Idea: By focusing on the top few PCs (e.g., PC1 and PC2), you can simplify complex data while preserving its most important patterns.
    Variance Explained
    The scree plot visualizes the variance contributed by each PC. Typically, the first 2–3 components explain most of the variance, allowing you to discard less informative dimensions without significant information loss.
    <br><b>How ARM is Used in the Project</b><br>
    We discuss how PCA is used in the project to reduce dimensionality, thus simplifying the dataset while retaining essential variation that informs crop prediction. The explanation highlights the significance of first few principal components and how PCA supports subsequent clustering and ARM by helping to visualize data in lower-dimensional spaces.   <br><br>Figure:Variance explained by each principal component. <br><img src="images\18.png" alt=""/><br><img src="images\21.jpg" alt=""/>
  </p>
  <p>
    <b>Data Preparation:</b><br>
    Data preparation for PCA involved standardizing the numerical variables since PCA relies on variances that are scale-sensitive. 
The code used for the PCA transformation is available via this link: <a href="https://github.com/larry6683/ML/blob/main/Crop%20recommendation.ipynb">https://github.com/yourrepo/pca_code.py</a>. The code leverages the scikit-learn library in Python and includes steps for standard normalization and the extraction of principal components. Detailed comments explain the variance retention rate and the choice of the number of components.
<br><br>Figure:The original high-dimensional data frame <br><img src="images\16.png" alt=""/><br>
Figure: The data after standardization and subsequent PCA transformation<br><img src="images\17.png" alt=""/>
</p>
  <p><b>Results</b><br>
    The PCA results are summarized by visualizing both 2D and 3D projections. First figure shows the 2D scatter plot of the first two principal components, while other figure presents an interactive 3D plot that reveals further data grouping.
Our analysis indicates that the first two principal components explain over 75% of the total variance, with a further component adding an additional 10% of variance. These results demonstrate that PCA effectively reduces dimensionality while retaining the essential information for subsequent analysis.
The most important principal components correspond to <b>soil nutrient content and weather variability</b>. In conclusion, PCA has played a key role in simplifying the data without sacrificing critical information. The dimensionality reduction further supports improved visualization and interpretation of clusters and associations, ultimately facilitating more robust decision-making in our crop recommendation problem.
  <br><br>Figure: 2D scatter plot<br><img src="images\19.png" alt=""/><br>
  <br>Figure: 3D plot<br><img src="images\20.png" alt=""/><br>
</p>
</div>

<!-- Clustering-->
<div id="clustering" class="tabcontent">
  <h3>Clustering Analysis</h3>
  <p><b>Overview:<br></b>

   Clustering and Distance Metrics Explained:
    Clustering is an unsupervised learning technique that groups data points into clusters based on similarity. The distance metric defines how similarity is quantified, playing a pivotal role in determining cluster quality.  
<br><b>How ARM is Used in the Project</b><br>
    1. Clustering: Groups data points so that intra-cluster points are more similar than inter-cluster points. For example, customer segmentation groups buyers by behavior or demographics.  
    <br>Figure: After Clustering Dataframe:<br><img src="images\27.png" alt=""><br>
    2. Distance Metrics:  <br>
      - **Euclidean Distance: Straight-line distance between two points, ideal for low-dimensional data:  <br>
      d = √[ (x2– x1)2 + (y2– y1)2]<br>
      - **Manhattan Distance**: Sum of absolute differences along each axis, robust to outliers:  <br>
      d = |x1 - x2| + |y1 - y2|<br>
      - **Cosine Similarity**: Measures the angle between vectors, useful for text/image embeddings.  <br>
      S _C(x, y) = x . y / ||x|| ×× ||y||<br>
    The choice of metric directly impacts cluster shapes. For instance, Euclidean distance works well for spherical clusters, while Manhattan suits grid-like structures.
    
  </p>
  <p>
    <b>Data Preparation:<br></b>
    Before clustering, the dataset was cleaned and normalized. In Figure below you can see a snapshot of the raw data frame and in another figure the data after normalization and feature engineering. We discretized continuous variables where necessary and encoded categorical variables for compatibility with clustering algorithms.
    An accessible link to our clustering code is provided here: <a href="https://github.com/larry6683/ML/blob/main/Crop%20recommendation.ipynb">https://github.com/yourrepo/clustering_code.py</a>. Our implementation in Python uses libraries such as scikit-learn for K-means and DBSCAN, and SciPy for hierarchical clustering. We include comments throughout the code to explain the transformation steps and the rationale for selected hyperparameters.
  <br><br>Figure: Before Clustering Dataframe:<br><img src="images\23.png" alt=""><br>
  <br>Figure: After Clustering Dataframe:<br><img src="images\22.png" alt=""><br>
</p>
  <p>
  <b>Results:<br></b>The clustering results are presented with multiple visualizations. We include a dendrogram image that shows the results of hierarchical clustering and additional cluster maps for different values of K. The results detail cluster compositions for K = 3, 4, and 5, and we evaluated these using the silhouette score—with the optimal K value being highlighted in the report. 
  We also compare results from K-means, hierarchical clustering, and DBSCAN, discussing strengths and weaknesses of each method in the context of our application.
  In conclusion, the clustering analysis revealed distinct patterns within the soil-weather dataset. Non-technically, these clusters indicate that certain fertility and moisture conditions tend to co-occur, thereby suggesting groups of environments that may favor specific crop types. The clustering outcomes guide further analysis and serve as a basis for integrating association rules and dimensionality reduction.
  <br><br>Figure: K-Means Clustering<br><img src="images\24.png" alt=""><br>
  <br>Figure: Hierarchical clustering dendrogram<br><img src="images\25.png" alt=""><br>
  <br>Figure: DBSCAN clusstering<br><img src="images\26.png" alt=""><br>
</div>

<!-- ARM-->
<div id="arm" class="tabcontent">
  <h3>ARM</h3>
  <p><b>Overview:<br></b>
    Association Rule Mining (ARM) is a technique used to uncover interesting relationships between variables in large datasets. It is commonly applied in market basket analysis, recommendation systems, and other domains to identify frequent itemsets and generate association rules.<br>
    <br><b>How ARM is Used in the Project</b><br>
In this project, ARM can be applied to the dataset to identify relationships between soil properties, weather conditions, and crop recommendations. For example:
Frequent Itemsets: Identify combinations of soil properties (e.g., pH, K, P) and weather conditions that frequently occur together.
Association Rules: Generate rules such as:
"If the soil pH is between 5.5 and 6.5 and the potassium level is high, then the recommended crop is Barley."
"If the weather condition is dry and the phosphorus level is low, then the recommended crop is Maize."
These insights can help in making data-driven decisions for crop recommendations based on soil and weather conditions.
<br><br>Figure: Association rules<br><img src="images\28.png" alt=""><br>
<br>Figure: Frequent Itemsets<br><img src="images\29.png" alt=""><br>
<br>Figure: Association rules: Lift vs Confidence<br><img src="images\30.png" alt=""><br>
  </p>
  <p>
  <b>Data preparation:<br></b>For association rule mining, the dataset was transformed into a transactional format where each transaction lists discrete items representing soil color, discretized pH values, nutrient levels, and crop labels. Figure below shows a sample of the original data frame while another displays the transformed transactional data.
  An accessible link to our ARM code is provided here: <a href="https://github.com/larry6683/ML/blob/main/Crop%20recommendation.ipynb">https://github.com/yourrepo/arm_code.py</a>. The code is implemented in Python using the mlxtend library which provides functions for one-hot encoding, mining frequent itemsets with the Apriori algorithm, and generating association rules based on minimum thresholds for support and confidence. Comments in the code explain each step of the processing pipeline clearly.
  <br>Figure: Association rules: Lift vs Confidence<br><img src="images\35.png" alt=""><br>
 </p>
  <p>
    <b>Results:<br></b>The ARM analysis produced a rich set of association rules. We filtered the results to report the top 15 rules sorted by support, confidence, and lift. In our code we set a minimum support threshold of 5% and a minimum confidence of 60%.
    A network visualization of the rules is provided below. In this image, nodes represent items and directed edges connect antecedent items to consequent items. The network layout highlights clusters of rules that show strong associations between certain soil properties and crop recommendations.
    In conclusion, the ARM analysis provided actionable insights into which combinations of soil attributes are most significant for predicting crop labels. Nontechnically, these results underscore common scenarios where certain soil conditions tend to favor particular crops, supporting recommendations for better farm management practices.
    <br>Figure: Top 15 rules by support<br><img src="images\31.png" alt=""><br>
    <br>Figure: Top 15 rules by lift<br><img src="images\32.png" alt=""><br>
    <br>Figure: Top 15 rules by confidence<br><img src="images\33.png" alt=""><br>
    <br>Figure: Association rules in network visualization<br><img src="images\34.png" alt=""><br>
  </p>
</div>

<!-- Naive Bayes -->
  <div id="naive-bayes" class="tabcontent">
    <h3>Supervised learning: Naive Bayes</h3>
    <h4>(a) Overview</h4>
Naive Bayes is a probabilistic classification algorithm based on Bayes' Theorem. It assumes that the features are independent of each other, which simplifies computation and makes it efficient for large datasets. Naive Bayes is widely used in text classification, spam filtering, sentiment analysis, and medical diagnosis.
<br>Types of Naive Bayes Classifiers:<br>
1. Gaussian Naive Bayes (GNB):<br>
Assumes that the features follow a Gaussian (normal) distribution.Suitable for continuous data.<br>
Example: Predicting crop suitability based on continuous soil properties like pH or nutrient levels.<br>
2. Multinomial Naive Bayes (MNB):<br>
Used for discrete data, such as word counts in text classification.<br>
Example: Classifying crops based on categorical soil types or weather conditions.<br>
3. Bernoulli Naive Bayes (BNB):<br>
Designed for binary/boolean features.<br>
Example: Predicting whether a crop is suitable (yes/no) based on binary soil conditions.<br>
4. Categorical Naive Bayes (CNB):<br>
Handles categorical data directly.<br>
Example: Classifying crops based on soil color or other categorical features.<br>
Comparison:<br>
<ul>
  <li>Gaussian NB is best for continuous data, while Multinomial NB and Bernoulli NB are better suited for discrete or binary data.</li>
  <li> Categorical NB is specifically designed for categorical features, making it ideal for datasets with non-numeric attributes.</li>
</ul>

    <br>
    <h4>(b) Data Prep:</h4>
    <p>
      1. Labeled Data: The dataset is labeled.<br>
2. Train-Test Split: The data is split into training and testing sets to evaluate model performance.<br>
<b>The split must be disjoint to avoid data leakage.</b><br>
3. Data Types: Different Naive Bayes flavors require different data formats.<br>
Gaussian NB: Continuous features.<br>
Multinomial NB: Discrete features.<br>
Bernoulli NB: Binary features.<br>
Categorical NB: Categorical features.
    </p>
    <a href='https://drive.google.com/file/d/1_Oachbx5s9PMW-li8AidlthSJlEQpBeX/view?usp=sharing' alt=''>Link to the data prepared</a><br>
    Image of the dataframe used.Dataframe too long to fit in this image.<br>
    <img src='images/36.jpeg' alt="dataframe"></img><br>
    Training Dataframe:(X_train)<br>
    <img src="images/37.png"/><br>
    Testing Dataframe:(X_test)<br>
    <img src="images/38.png"/><br>
 
   <h4>(c) Code:</h4> 
<p>   Link to the Code: <a href="https://github.com/larry6683/ML/blob/main/Crop%20Recommendation%20using%20Soil%20Properties%20and%20Weather%20Prediction.ipynb">https://github.com/larry6683/ML/blob/main/Crop%20Recommendation%20using%20Soil%20Properties%20and%20Weather%20Prediction.ipynb</a></p><br>

<h4>(d) Results:</h4>
Accuracy Scores: Comparing the accuracy of the models.<br>
Confusion Matrices: The confusion matrices for each model to visualize true positives, false positives, true negatives, and false negatives.<br>
Observations:<br>
1. Gaussian NB performs well with continuous features like pH and nutrient levels.<br>
<img src="images/39.png"/><br>   <img src="images/43.png"/><br>
2. Multinomial NB is effective for discrete features like soil type.<br>
<img src="images/40.png"/><br>   <img src="images/44.png"/><br>
3. Bernoulli NB.<br>
<img src="images/41.png"/><br>   <img src="images/45.png"/><br>
4. Categorical NB is ideal for categorical features like soil color.<br>
<img src="images/42.png"/><br>   <img src="images/46.png"/><br>

<h4>(e) Conclusions</h4>
Naive Bayes is a simple yet reliable classifier. It would be fine if we have the assumption of independence. Gaussian NB was suitable for continuous features, and Multinomial NB was suitable for discrete features, while Categorical NB was suitable for categorical features. 
The models can be utilized to classify crop suitability based on soil and climate information to help farmers make intelligent decisions.

  </div>

<!-- Decision Tree -->
<div id="decision-tree" class="tabcontent">
  <h3>Supervised learning: Decision Trees</h3>
  <h4>(a) Overview</h4>

  Decision Trees (DTs) are a classification as well as regression method used in supervised learning. DTs partition the data according to feature values and construct a tree-based model. DTs are interpretable in nature and are able to process categorical as well as continuous data.
 <br><b> Main Ideas:</b><br>
  Gini Index: This is a measure of a node's impurity. In Gini, lower is desirable.<br>
  Entropy: It quantifies the randomness of information. Low entropy implies good splits.<br>
  Information Gain: The reduction of entropy after a split. The greater the information gain, the better the split.<br>
  Example of Gini and Information Gain:<br>
  I choose "Barley" and "Bean".<br>
  Gini Index before split: 0.5.<br>
  Gini Index after split: 0.2<br>
  Information Gain = 0.5 - 0.2 = 0.3.<br>
  Infinite Trees:<br>
  We can generate an unlimited number of trees by repeatedly partitioning the data. This will result in overfitting. We can prevent this by limiting tree depth or pruning.<br>
  <br>
  <h4>(b) Data Prep:</h4>
  <ul>
    <li>Using the same dataset as Naive Bayes.</li>
    <li>Spliting the data into training and testing sets.</li>
    <li> Ensure the split is disjoint to avoid data leakage.</li>
  </ul>
  Image of the dataframe used.Dataframe too long to fit in this image.<br>
  <img src='images/36.jpeg' alt="dataframe"></img><br>
  <br>
 <h4>(c) Code:</h4> 
<p>   Link to the Code: <a href="https://github.com/larry6683/ML/blob/main/Crop%20Recommendation%20using%20Soil%20Properties%20and%20Weather%20Prediction.ipynb">https://github.com/larry6683/ML/blob/main/Crop%20Recommendation%20using%20Soil%20Properties%20and%20Weather%20Prediction.ipynb</a></p><br>

 <br>

 <h4>(d) Results:</h4>
 1. Accuracy Scores: Comparing the accuracy of the trees.<br>
 2. Confusion Matrices: Displaying the confusion matrices for each tree.<br>
 3. Tree Visualizations: Include visualizations of at least three different trees with different root nodes.<br>
 Observations:<br>
 <h5>Decision Tree 1 (gini, max_depth=5)</h5>
Evaluation Metrics:<br>
<img src="images/48.png"/><br>
Decision tree plot:<br>
<img src="images/49.png"/><br>
 <h5>Decision Tree 2 (gini, max_depth=3)</h5>
 Evaluation Metrics:<br>
 <img src="images/50.png"/><br>
 Decision tree plot:<br>
 <img src="images/51.png"/><br>
 <h5>Decision Tree 3 (entropy, max_depth=4)</h5>
 Evaluation Metrics:<br>
 <img src="images/52.png"/><br>
 Decision tree plot:<br>
 <img src="images/53.png"/><br>
Max_depth =4 and criterion = entropy gave the highest accuracy.<br>
 <br>
<h4>(e) Conclusions</h4>
Decision Trees are explainable and are good for both continuous and categorical data. Decision Trees can be applied to discover the most influential features in crop suitability prediction. 
They overfit, but it is possible to evade it by limiting tree depth or by pruning.
</div>

<!-- Regression-->
<div id="regression" class="tabcontent">
  <h3>Supervised learning: Regression</h3>
  <h4>(a) Define and explain linear regression.</h4>
  Linear regression is a supervised learning model used to predict a continuous target variable. It illustrates the relationship between the target variable and one or more feature variables by fitting the data into a linear equation.
  <h4>(b) Define and explain logistic regression.</h4>
  Logistic regression is a supervised classifier for binary classification tasks. Logistic regression predicts a class probability through the use of the sigmoid function, which scales predictions onto the range 0 to 1.
  <h4>(c) In how many ways are they alike, and in how many are they different?</h4>
  Similarities:
  Both are linear models.
  Both utilize optimization routines to reduce loss functions.
  Differences:
  Linear regression predicts continuous values, but logistic regression predicts probabilities in classification.
  Logistic regression uses the sigmoid function but linear regression does not.
  <h4>(d) Does logistic regression use the Sigmoid function? Why?</h4>
  Yes, logistic regression uses the sigmoid function to map predictions onto probabilities between 0 and 1. The sigmoid function ensures that the output can be understood as a probability.
  <h4>(e) Explain how maximum likelihood is connected to logistic regression.</h4>
  Logistic regression uses maximum likelihood estimation (MLE) to find parameters that maximize the likelihood of observed data. MLE guarantees the model's prediction is as close to the true data as possible.
<br><br>
<b>Simple comparision of all 3 models:</b><br>
1. Naive Bayes: Performs well with categorical and discrete features.<br>
2. Decision Trees: Effective for both categorical and continuous data but prone to overfitting.<br>
3. Logistic Regression: Ideal for binary classification tasks with continuous features.<br>

<h4>Coding: Comparing Logistic regression analysis and Multinomial Naive Bayes Classifiers</h4>
<p>   Link to the Code: <a href="https://github.com/larry6683/ML/blob/main/Crop%20Recommendation%20using%20Soil%20Properties%20and%20Weather%20Prediction.ipynb">https://github.com/larry6683/ML/blob/main/Crop%20Recommendation%20using%20Soil%20Properties%20and%20Weather%20Prediction.ipynb</a></p><br>
Image of the updated dataframe of two labels (Beans and Barley).<br>
<img src="images/54.png"/><br>
1.Logistic regression for updated 2 label dataset: Accuracy, classification report and confusion matrix below.<br>
<img src="images/55.png"/><br>
<img src="images/56.png"/><br>
2. Naive bayes for updated 2 label dataset: Accuracy, classification report and confusion matrix below.<br>
<img src="images/57.png"/><br>
<img src="images/58.png"/><br>

</div>


  <!-- JavaScript to handle tab switching -->
  <script>
    function showTab(tabName) {
      var tabcontents = document.getElementsByClassName("tabcontent");
      for (var i = 0; i < tabcontents.length; i++) {
        tabcontents[i].style.display = "none";
      }
      document.getElementById(tabName).style.display = "block";
    }
    window.onload = function() {
      showTab('introduction');
    };
  </script>
</body>
</html>
